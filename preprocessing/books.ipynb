{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T13:44:45.791158Z",
     "start_time": "2019-08-08T13:44:41.221241Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import chisquare\n",
    "from scipy import stats\n",
    "import urllib.request\n",
    "import string\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "import gensim\n",
    "import sys\n",
    "import spacy\n",
    "nlp=spacy.load('en')\n",
    "import seaborn as sns\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tknzr = WordPunctTokenizer()\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from scipy import spatial\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "from gensim import corpora\n",
    "import re\n",
    "import logging\n",
    "import requests\n",
    "import urllib \n",
    "import re\n",
    "import sys\n",
    "import zipfile\n",
    "from glob import glob\n",
    "import statsmodels.api as sm\n",
    "import scipy\n",
    "import scipy.stats as ss\n",
    "from scipy.signal import savgol_filter\n",
    "import random\n",
    "# np.set_printoptions(precision=4)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "# from the demo\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import names\n",
    "pd.set_option('display.max_columns',None)\n",
    "pd.set_option('display.max_rows',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T13:44:45.796979Z",
     "start_time": "2019-08-08T13:44:45.793145Z"
    }
   },
   "outputs": [],
   "source": [
    "def flushPrint(s):\n",
    "    sys.stdout.write('\\r')\n",
    "    sys.stdout.write('%s' % s)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T13:44:47.729428Z",
     "start_time": "2019-08-08T13:44:47.711168Z"
    }
   },
   "outputs": [],
   "source": [
    "# 获取名字，性别数据列表\n",
    "from nltk.corpus import names\n",
    "male_names = [name for name in names.words('male.txt')]\n",
    "female_names = [name for name in names.words('female.txt')]\n",
    "names = male_names+female_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T13:44:48.358391Z",
     "start_time": "2019-08-08T13:44:48.349158Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('./stopword.txt','r') as f:\n",
    "    stopword=f.readlines()\n",
    "stopword=[i.split('\\n')[0] for i in stopword]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T13:44:49.234878Z",
     "start_time": "2019-08-08T13:44:49.225193Z"
    }
   },
   "outputs": [],
   "source": [
    "files=glob('./gutenberg/corpus/*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T13:44:52.326204Z",
     "start_time": "2019-08-08T13:44:52.289219Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_maintext_lines_gutenberg(raw_text):\n",
    "    lines = raw_text.split(\"\\n\")\n",
    "    start_book_i = 0\n",
    "    end_book_i = len(lines)-1\n",
    "    # pass 1, this format is easy and gets 78.9% of books\n",
    "    start1=\"START OF THIS PROJECT GUTENBERG EBOOK\"\n",
    "    start2=\"START OF THE PROJECT GUTENBERG EBOOK\"\n",
    "    end1=\"END OF THIS PROJECT GUTENBERG EBOOK\"\n",
    "    end2=\"END OF THE PROJECT GUTENBERG EBOOK\"\n",
    "    end3=\"END OF PROJECT GUTENBERG\"\n",
    "    for j,line in enumerate(lines):\n",
    "        if (start1 in line) or (start2 in line):\n",
    "            # and \"***\" in line and start_book[i] == 0 and j<.25*len(lines):\n",
    "            start_book_i = j\n",
    "        end_in_line = end1 in line or end2 in line or end3 in line.upper()\n",
    "        if end_in_line and (end_book_i == (len(lines)-1)):\n",
    "            #  and \"***\" in line and j>.75*len(lines)\n",
    "            end_book_i = j\n",
    "    # pass 2, this will bring us to 99%\n",
    "    if (start_book_i == 0) and (end_book_i == len(lines)-1):\n",
    "        for j,line in enumerate(lines):\n",
    "            if (\"end\" in line.lower() or \"****\" in line) and  \"small print\" in line.lower() and j<.5*len(lines):\n",
    "                start_book_i = j\n",
    "            if \"end\" in line.lower() and \"project gutenberg\" in line.lower() and j>.75*len(lines):\n",
    "                end_book_i = j\n",
    "        # pass three, caught them all (check)\n",
    "        if end_book_i == len(lines)-1:\n",
    "            for j,line in enumerate(lines):\n",
    "                if \"THE END\" in line and j>.9*len(lines):\n",
    "                    end_book_i = j\n",
    "    return lines[(start_book_i+1):(end_book_i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T13:44:58.031528Z",
     "start_time": "2019-08-08T13:44:53.616822Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"diction.txt\",\"r\") as f:\n",
    "    ediction = json.loads(f.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T13:45:01.020276Z",
     "start_time": "2019-08-08T13:45:00.989771Z"
    }
   },
   "outputs": [],
   "source": [
    "def chunkify(lines):\n",
    "    # put them back together...\n",
    "    raw_text = \"\\n\".join(lines)\n",
    "    # remove extra whitespace\n",
    "    raw_text_1 = re.sub(\"\\n[\\\\s]+?\\n\",\"\\n\\n\",raw_text)\n",
    "    # remove singleton newlines\n",
    "    raw_text_2 = re.sub(r\"([^\\n])\\n([^\\n])\",r\"\\1 \\2\",raw_text_1)\n",
    "    raw_text_3 = raw_text_2.rstrip().lstrip()\n",
    "\n",
    "    # three levels of significance\n",
    "    # single newlines were already discarded (insignificant)\n",
    "    # double newlines are real line breaks\n",
    "    # triple newlines (or more) separate content\n",
    "\n",
    "    # split on those triples (or more)\n",
    "    big_chunks = re.split(\"\\n\\n\\n+\",raw_text_3)\n",
    "\n",
    "    # now break them into the paragraphs\n",
    "    small_chunks = list(map(lambda x: re.split(\"\\n\\n\",x),big_chunks))\n",
    "    combined_chunks = []\n",
    "    [combined_chunks.extend(el) for el in small_chunks]\n",
    "#     combined_chunks = []\n",
    "\n",
    "#     for i in range(len(small_chunks)):\n",
    "#         for j in range(len(small_chunks[i])):\n",
    "#             combined_chunks.append((i,j,small_chunks[i][j]))\n",
    "    return combined_chunks\n",
    "#     return small_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### charcter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T14:02:19.590831Z",
     "start_time": "2019-08-05T14:02:19.572224Z"
    }
   },
   "outputs": [],
   "source": [
    "def main_character(lead):\n",
    "#     lead=[i.title() for i in lead]\n",
    "    film_name_list = (set(lead) & set(male_names)) |\\\n",
    "                    (set(lead) & set(female_names))\n",
    "    film_script_wordlist_counter = Counter(lead)\n",
    "    df = pd.DataFrame([[i, film_script_wordlist_counter[i]] for i in film_name_list]\\\n",
    "                                      , columns=['name', 'count'])\n",
    "    df['gender']=df['name'].apply(lambda x:'male' if x in male_names else 'female')\n",
    "    df=df.sort_values(by='count', ascending=False)\n",
    "    lead=df[df['gender']=='male']['name'].values[0]\n",
    "    lead_n=df[df['gender']=='male']['count'].values[0]\n",
    "    lead_gender=df['gender'].values[0]\n",
    "    lead1=df[df['gender']=='female']['name'].values[0]\n",
    "    lead1_n=df[df['gender']=='female']['count'].values[0]\n",
    "    return lead,lead_n,lead1,lead1_n,lead_gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T14:27:02.245850Z",
     "start_time": "2019-08-05T14:27:01.659905Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9"
     ]
    }
   ],
   "source": [
    "characters=[]\n",
    "for m,n in enumerate(files[:10]):\n",
    "    flushPrint(m) \n",
    "    try:\n",
    "        f = open(n,\"r\",encoding='ISO-8859-1')\n",
    "        rawtext = f.read()\n",
    "        f.close()\n",
    "        lines = get_maintext_lines_gutenberg(rawtext)\n",
    "        chunked = chunkify(lines)\n",
    "        text=tknzr.tokenize(rawtext)\n",
    "        characters.append([n.split('/')[-1].split('.')[0],main_character(text)\\\n",
    "                          ,len(text),len(chunked)])\n",
    "    except Exception as e:\n",
    "        print(e,n)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T14:22:00.219840Z",
     "start_time": "2019-08-05T14:22:00.216752Z"
    }
   },
   "outputs": [],
   "source": [
    "data=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T14:27:32.655175Z",
     "start_time": "2019-08-05T14:27:32.648734Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['id']=[i[0] for i in characters]\n",
    "data['name']=[i[1] for i in characters]\n",
    "data['word']=[i[2] for i in characters]\n",
    "data['para']=[i[3] for i in characters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T14:28:01.709907Z",
     "start_time": "2019-08-05T14:28:01.705408Z"
    }
   },
   "outputs": [],
   "source": [
    "data.to_csv('./all_books.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### all_books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T08:34:32.684436Z",
     "start_time": "2019-08-06T08:34:31.809964Z"
    }
   },
   "outputs": [],
   "source": [
    "book=pd.read_csv('./all_books.csv')\n",
    "book['no']=book['id'].apply(lambda x: x.split('-')[0])\n",
    "book=book.drop(['id'],axis=1)\n",
    "book=book.drop_duplicates()\n",
    "book['name']=book['name'].apply(lambda x: eval(x))\n",
    "book=book[book['no']!='0']\n",
    "book['character']=book['name'].apply(lambda x: [x[0],x[2],x[-1]])\n",
    "book['lead_gender']=book['name'].apply(lambda x: x[-1])\n",
    "book['name_c']=book['name'].apply(lambda x: x[1])\n",
    "book['name_c1']=book['name'].apply(lambda x: x[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T08:34:33.872193Z",
     "start_time": "2019-08-06T08:34:33.080558Z"
    }
   },
   "outputs": [],
   "source": [
    "metadata=pd.read_csv('./gutenberg/metadata.csv')\n",
    "metadata=metadata[metadata['lang']=='English']\n",
    "metadata['class']=metadata['loc_class'].apply(lambda x: [i[:2] for i in x.split('|') if i[0] in [str(chr(i)) for i in range(65,91)]])\n",
    "metadata=metadata.drop_duplicates(subset=['no'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T08:36:43.023060Z",
     "start_time": "2019-08-06T08:36:42.841946Z"
    }
   },
   "outputs": [],
   "source": [
    "all_book=pd.merge(metadata,book,on=['no'])\n",
    "all_book=all_book.drop_duplicates(subset=['no'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T08:08:59.204814Z",
     "start_time": "2019-08-06T08:08:59.194355Z"
    }
   },
   "outputs": [],
   "source": [
    "choose=all_book[(all_book['name_c']>10)&(all_book['name_c1']>10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T08:08:59.902674Z",
     "start_time": "2019-08-06T08:08:59.898534Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29871"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(choose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T08:06:06.297192Z",
     "start_time": "2019-08-06T08:06:06.288705Z"
    }
   },
   "outputs": [],
   "source": [
    "def word_to_emotion(data):\n",
    "    d=[]\n",
    "    for word in tknzr.tokenize(data):\n",
    "        try:\n",
    "            if (word.lower() not in stopword):#&(word not in names):\n",
    "                sim = ediction[word]\n",
    "                if (sim<-3)|(sim>3):\n",
    "                    d.append(sim)\n",
    "        except:\n",
    "            pass\n",
    "    return np.mean(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T07:42:39.948599Z",
     "start_time": "2019-08-06T07:42:06.094285Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xuhuimin/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/xuhuimin/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/18520.txt\n",
      "8index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/91-0.txt\n",
      "9index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/7353.txt\n",
      "10index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/24436.txt\n",
      "12index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/2607.txt\n",
      "19index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/8920.txt\n",
      "21index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/10966.txt\n",
      "23index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/36-0.txt\n",
      "25index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/21775.txt\n",
      "26index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/847.txt\n",
      "31index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/4272.txt\n",
      "32index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/116-readme.txt\n",
      "34index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/5230.txt\n",
      "37index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/13897.txt\n",
      "38index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/4099.txt\n",
      "40index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/1282.txt\n",
      "42index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/1094.txt\n",
      "48index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/5083.txt\n",
      "50index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/18768.txt\n",
      "52index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/40-readme.txt\n",
      "54index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/20989.txt\n",
      "55index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/119-0.txt\n",
      "59index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/2260.txt\n",
      "61index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/10337.txt\n",
      "62index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/24723.txt\n",
      "63index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/30742.txt\n",
      "65index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/9297.txt\n",
      "66index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/37992.txt\n",
      "67index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/28118.txt\n",
      "68index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/834.txt\n",
      "69index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/32746.txt\n",
      "72index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/147-0.txt\n",
      "73index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/6985.txt\n",
      "74index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/10736.txt\n",
      "76index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/213.txt\n",
      "77index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/47530.txt\n",
      "78index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/70-0.txt\n",
      "79index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/40504.txt\n",
      "84index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/37758.txt\n",
      "91index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/13783.txt\n",
      "93index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/10671.txt\n",
      "95index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/1-0.txt\n",
      "100index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/43-0.txt\n",
      "102index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/35425.txt\n",
      "104index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/20559.txt\n",
      "107index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/1013.txt\n",
      "109index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/14203.txt\n",
      "111index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/9846.txt\n",
      "112index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/6340.txt\n",
      "114index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/3006.txt\n",
      "116index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/421.txt\n",
      "117index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/1837.txt\n",
      "126index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/14360.txt\n",
      "127index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/2496.txt\n",
      "129index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/19651.txt\n",
      "133index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/2911.txt\n",
      "141index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/1059.txt\n",
      "142index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/18846.txt\n",
      "143index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/687.txt\n",
      "150index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/41715.txt\n",
      "153index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/4087.txt\n",
      "155index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/126.txt\n",
      "160index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/534.txt\n",
      "162index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/177-0.txt\n",
      "165index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/20919.txt\n",
      "166index 0 is out of bounds for axis 0 with size 0 ./gutenberg/corpus/21959.txt\n"
     ]
    }
   ],
   "source": [
    "characters=[]\n",
    "for m,n in enumerate(files):\n",
    "    flushPrint(m)\n",
    "    try:\n",
    "        f = open(n,\"r\",encoding='ISO-8859-1')\n",
    "        rawtext = f.read()\n",
    "        f.close()\n",
    "        index=n.split('/')[-1].split('.')[0].split('-')[0]\n",
    "        lines = get_maintext_lines_gutenberg(rawtext)\n",
    "        chunked = chunkify(lines)\n",
    "        lead=choose[choose['no']==index]['character'].values[0][0]\n",
    "        lead1=choose[choose['no']==index]['character'].values[0][1]\n",
    "        tp=[i for i,j in enumerate(chunked) if (lead in tknzr.tokenize(j)) & (lead1 in tknzr.tokenize(j))]\n",
    "        m=[i for i,j in enumerate(chunked) if lead in tknzr.tokenize(j)]\n",
    "        f=[i for i,j in enumerate(chunked) if lead1 in tknzr.tokenize(j)]\n",
    "        w=[word_to_emotion(i) for i in chunked]\n",
    "        characters.append([index,tp,m,f,w])\n",
    "    except Exception as e:\n",
    "        print(e,n)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T07:53:33.090543Z",
     "start_time": "2019-08-06T07:53:33.087533Z"
    }
   },
   "outputs": [],
   "source": [
    "data=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T08:08:04.047018Z",
     "start_time": "2019-08-06T08:08:04.039665Z"
    }
   },
   "outputs": [],
   "source": [
    "data['index']=[i[0] for i in characters]\n",
    "data['tp']=[i[1] for i in characters]\n",
    "data['f']=[i[2] for i in characters]\n",
    "data['m']=[i[3] for i in characters]\n",
    "data['w']=[i[4] for i in characters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T08:10:26.074899Z",
     "start_time": "2019-08-06T08:10:25.860635Z"
    }
   },
   "outputs": [],
   "source": [
    "data.to_csv('./all_books_emotion.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T08:14:34.796968Z",
     "start_time": "2019-08-06T08:14:34.791704Z"
    }
   },
   "outputs": [],
   "source": [
    "index=[i[0] for i in characters]\n",
    "tp=[i[1] for i in characters]\n",
    "m=[i[2] for i in characters]\n",
    "f=[i[3] for i in characters]\n",
    "w=[i[4] for i in characters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T08:15:27.258128Z",
     "start_time": "2019-08-06T08:15:27.253919Z"
    }
   },
   "outputs": [],
   "source": [
    "diction1=dict(zip(index,tp))\n",
    "diction2=dict(zip(index,m))\n",
    "diction3=dict(zip(index,f))\n",
    "diction4=dict(zip(index,w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T08:17:15.386029Z",
     "start_time": "2019-08-06T08:17:14.925211Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xuhuimin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/xuhuimin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/Users/xuhuimin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/xuhuimin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "choose['tp']=choose['no'].apply(lambda x: diction1[x] if x in diction1 else np.nan)\n",
    "choose['m']=choose['no'].apply(lambda x: diction2[x] if x in diction2 else np.nan)\n",
    "choose['f']=choose['no'].apply(lambda x: diction3[x] if x in diction3 else np.nan)\n",
    "choose['w']=choose['no'].apply(lambda x: diction4[x] if x in diction4 else np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T08:19:36.340178Z",
     "start_time": "2019-08-06T08:19:35.563044Z"
    }
   },
   "outputs": [],
   "source": [
    "choose.to_csv('./choose.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
